{"cells":[{"cell_type":"markdown","metadata":{"id":"GXcF1Ka5Ew7V"},"source":["# Training the adapters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EuTxENL2FtDX","outputId":"8225b0b8-a5bd-4304-c06b-e1f15bb6bb73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: adapters in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","\n","Requirement already satisfied: transformers~=4.39.3 in /usr/local/lib/python3.10/dist-packages (from adapters) (4.39.3)\n","\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (3.14.0)\n","\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (0.20.3)\n","\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (1.25.2)\n","\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (24.0)\n","\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (6.0.1)\n","\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (2023.12.25)\n","\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (2.31.0)\n","\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (0.15.2)\n","\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (0.4.3)\n","\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3->adapters) (4.66.4)\n","\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.39.3->adapters) (2023.6.0)\n","\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers~=4.39.3->adapters) (4.11.0)\n","\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3->adapters) (3.3.2)\n","\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3->adapters) (3.7)\n","\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3->adapters) (2.0.7)\n","\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3->adapters) (2024.2.2)\n"]}],"source":["!pip install adapters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEg5vXptGLJ9","outputId":"54b101d1-42ab-452a-9156-c076d4d07115"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.39.3)\n","\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n","\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n","\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n","\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.30.1)\n","\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n","\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n","\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n","\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n","\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n","\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n","\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n","\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n","\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n","\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n","\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n","\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"]}],"source":["!pip install transformers[torch]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g8oGuwX-lwFK","outputId":"f85f37ed-fe78-4a12-f57d-8e0e9eb3874d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting datasets\n","\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","\n","Collecting xxhash (from datasets)\n","\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\n","\u001b[?25hCollecting multiprocess (from datasets)\n","\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","\n","  Attempting uninstall: huggingface-hub\n","\n","    Found existing installation: huggingface-hub 0.20.3\n","\n","    Uninstalling huggingface-hub-0.20.3:\n","\n","      Successfully uninstalled huggingface-hub-0.20.3\n","\n","Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"]}],"source":["!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"acpq9omhDbJG","outputId":"2f27ac79-6096-4394-8c53-2947e4aeb977"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading model and tokenizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","\n","  warnings.warn(\n","\n","Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['heads.default.3.bias']\n","\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Loading adapter...\n","\n","Adding new classification head...\n","\n","Loading datasets...\n","\n","Tokenizing datasets...\n","\n","Setting up adapter training...\n","\n","Training the model...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","\n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 14:44, Epoch 20/20]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.738700</td>\n","      <td>0.652630</td>\n","      <td>0.765657</td>\n","      <td>0.768536</td>\n","      <td>0.778379</td>\n","      <td>0.765657</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.528800</td>\n","      <td>0.586183</td>\n","      <td>0.777778</td>\n","      <td>0.764318</td>\n","      <td>0.796738</td>\n","      <td>0.777778</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.471900</td>\n","      <td>0.565858</td>\n","      <td>0.797980</td>\n","      <td>0.792773</td>\n","      <td>0.801667</td>\n","      <td>0.797980</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.428000</td>\n","      <td>0.605459</td>\n","      <td>0.793939</td>\n","      <td>0.795181</td>\n","      <td>0.799421</td>\n","      <td>0.793939</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.417400</td>\n","      <td>0.712645</td>\n","      <td>0.787879</td>\n","      <td>0.781033</td>\n","      <td>0.792488</td>\n","      <td>0.787879</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.374200</td>\n","      <td>0.720113</td>\n","      <td>0.781818</td>\n","      <td>0.778311</td>\n","      <td>0.782520</td>\n","      <td>0.781818</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.343300</td>\n","      <td>0.662482</td>\n","      <td>0.808081</td>\n","      <td>0.806699</td>\n","      <td>0.807054</td>\n","      <td>0.808081</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.325600</td>\n","      <td>0.777551</td>\n","      <td>0.802020</td>\n","      <td>0.799951</td>\n","      <td>0.800911</td>\n","      <td>0.802020</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.288100</td>\n","      <td>0.966474</td>\n","      <td>0.787879</td>\n","      <td>0.785589</td>\n","      <td>0.786512</td>\n","      <td>0.787879</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.260100</td>\n","      <td>0.884112</td>\n","      <td>0.783838</td>\n","      <td>0.783084</td>\n","      <td>0.782935</td>\n","      <td>0.783838</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.250200</td>\n","      <td>0.835092</td>\n","      <td>0.802020</td>\n","      <td>0.799235</td>\n","      <td>0.801150</td>\n","      <td>0.802020</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.244800</td>\n","      <td>0.950366</td>\n","      <td>0.793939</td>\n","      <td>0.792214</td>\n","      <td>0.792665</td>\n","      <td>0.793939</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.207900</td>\n","      <td>0.990758</td>\n","      <td>0.787879</td>\n","      <td>0.787899</td>\n","      <td>0.787935</td>\n","      <td>0.787879</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.209100</td>\n","      <td>1.080919</td>\n","      <td>0.789899</td>\n","      <td>0.788423</td>\n","      <td>0.788702</td>\n","      <td>0.789899</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.190600</td>\n","      <td>1.142277</td>\n","      <td>0.797980</td>\n","      <td>0.796645</td>\n","      <td>0.797081</td>\n","      <td>0.797980</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.168200</td>\n","      <td>1.149443</td>\n","      <td>0.795960</td>\n","      <td>0.794319</td>\n","      <td>0.794399</td>\n","      <td>0.795960</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.167400</td>\n","      <td>1.234795</td>\n","      <td>0.781818</td>\n","      <td>0.778251</td>\n","      <td>0.781925</td>\n","      <td>0.781818</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.167300</td>\n","      <td>1.252769</td>\n","      <td>0.789899</td>\n","      <td>0.788039</td>\n","      <td>0.788559</td>\n","      <td>0.789899</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.153500</td>\n","      <td>1.267215</td>\n","      <td>0.785859</td>\n","      <td>0.784971</td>\n","      <td>0.784988</td>\n","      <td>0.785859</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.150200</td>\n","      <td>1.266801</td>\n","      <td>0.785859</td>\n","      <td>0.784752</td>\n","      <td>0.784606</td>\n","      <td>0.785859</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["New best F1 score: 0.768536008159265. Saving model...\n","\n","Evaluating model on the test set...\n","\n","scores on test set for best model: {'eval_loss': 0.6267037391662598, 'eval_accuracy': 0.763962494904199, 'eval_f1': 0.7666767249798707, 'eval_precision': 0.7749034155172728, 'eval_recall': 0.763962494904199}\n","\n","New best F1 score: 0.7927726790814368. Saving model...\n","\n","Evaluating model on the test set...\n","\n","scores on test set for best model: {'eval_loss': 0.5124924778938293, 'eval_accuracy': 0.8030982470444353, 'eval_f1': 0.7997645135800788, 'eval_precision': 0.8037457999686906, 'eval_recall': 0.8030982470444353}\n","\n","New best F1 score: 0.795181159995559. Saving model...\n","\n","Evaluating model on the test set...\n","\n","scores on test set for best model: {'eval_loss': 0.5370152592658997, 'eval_accuracy': 0.8116591928251121, 'eval_f1': 0.8134323047458849, 'eval_precision': 0.8187264170998542, 'eval_recall': 0.8116591928251121}\n","\n","New best F1 score: 0.80669857392439. Saving model...\n","\n","Evaluating model on the test set...\n","\n","scores on test set for best model: {'eval_loss': 0.5815584659576416, 'eval_accuracy': 0.8214431308601712, 'eval_f1': 0.8213188543702957, 'eval_precision': 0.8212972801535903, 'eval_recall': 0.8214431308601712}\n","\n","Saving the final model...\n","\n","Evaluating the model...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1228' max='614' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [614/614 00:37]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["New best F1 score: 0.8200014108949054. Saving model...\n","\n","Evaluating model on the test set...\n","\n","scores on test set for best model: {'eval_loss': 1.0692464113235474, 'eval_accuracy': 0.8196086424785977, 'eval_f1': 0.8200014108949054, 'eval_precision': 0.8206066253355287, 'eval_recall': 0.8196086424785977, 'eval_runtime': 19.2327, 'eval_samples_per_second': 255.087, 'eval_steps_per_second': 31.925, 'epoch': 20.0}\n"]},{"data":{"text/plain":["{'eval_loss': 1.0692464113235474,\n"," 'eval_accuracy': 0.8196086424785977,\n"," 'eval_f1': 0.8200014108949054,\n"," 'eval_precision': 0.8206066253355287,\n"," 'eval_recall': 0.8196086424785977,\n"," 'eval_runtime': 18.1661,\n"," 'eval_samples_per_second': 270.063,\n"," 'eval_steps_per_second': 33.799,\n"," 'epoch': 20.0}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertTokenizer, TrainingArguments, TrainerCallback\n","from adapters import AutoAdapterModel\n","import adapters\n","from datasets import load_dataset\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import numpy as np\n","\n","# Define the task\n","task = \"NLI\"  # Change this to \"SentimentAnalysis\" for sentiment analysis task\n","\n","print(\"Loading model and tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n","\n","print(\"Loading adapter...\")\n","adapter_path = \"/content/Adapter\" if task == \"NLI\" else \"/kaggle/working/Adapter\"\n","adapter = model.load_adapter(adapter_path, with_head=False)\n","\n","print(\"Adding new classification head...\")\n","# Add a new classification head for the new adapter\n","num_labels = 3 if task == \"NLI\" else 2\n","model.add_classification_head(f\"{task}_Head\", num_labels=num_labels)\n","\n","model.set_active_adapters(adapter)\n","\n","print(\"Loading datasets...\")\n","if task == \"NLI\":\n","    # load maximedb/sick_nl dataset\n","    train_dataset = load_dataset(\"maximedb/sick_nl\", split=\"train\")\n","    test_dataset = load_dataset(\"maximedb/sick_nl\", split=\"test\")\n","    validation_dataset = load_dataset(\"maximedb/sick_nl\", split=\"validation\")\n","    columns_to_keep = ['sentence_A', 'sentence_B', 'label']\n","else:\n","    # Load DBRD dataset\n","    train_dataset = load_dataset(\"dbrd\", split=\"train\")\n","    test_dataset = load_dataset(\"dbrd\", split=\"test\")\n","    train_valid_split = train_dataset.train_test_split(test_size=0.1)\n","    train_dataset = train_valid_split['train']\n","    validation_dataset = train_valid_split['test']\n","    columns_to_keep = ['text', 'label']\n","\n","# Get the column names\n","column_names = train_dataset.column_names\n","columns_to_remove = [col for col in column_names if col not in columns_to_keep]\n","\n","# Remove unnecessary columns\n","train_dataset = train_dataset.remove_columns(columns_to_remove)\n","test_dataset = test_dataset.remove_columns(columns_to_remove)\n","validation_dataset = validation_dataset.remove_columns(columns_to_remove)\n","\n","# Tokenize the datasets and keep the 'labels' field\n","print(\"Tokenizing datasets...\")\n","if task == \"NLI\":\n","    train_dataset = train_dataset.map(lambda example: {**tokenizer(example['sentence_A'], example['sentence_B']), \"labels\": example[\"label\"]}, batched=True)\n","    test_dataset = test_dataset.map(lambda example: {**tokenizer(example['sentence_A'], example['sentence_B']), \"labels\": example[\"label\"]}, batched=True)\n","    validation_dataset = validation_dataset.map(lambda example: {**tokenizer(example['sentence_A'], example['sentence_B']), \"labels\": example[\"label\"]}, batched=True)\n","else:\n","    max_length = 512\n","    train_dataset = train_dataset.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length', max_length=max_length), batched=True)\n","    test_dataset = test_dataset.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length', max_length=max_length), batched=True)\n","    validation_dataset = validation_dataset.map(lambda example: tokenizer(example['text'], truncation=True, padding='max_length', max_length=max_length), batched=True)\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n","    acc = accuracy_score(labels, predictions)\n","\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","# Custom callback to save the best model based on F1 score and evaluate the best model\n","class SaveBestModelCallback(TrainerCallback):\n","    def __init__(self, test_dataset):\n","        self.best_f1 = 0.0\n","        self.test_dataset = test_dataset\n","        self.testing = False\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        current_f1 = None\n","        try:\n","            metrics = kwargs['metrics']\n","            current_f1 = metrics.get('eval_f1')\n","        except Exception as e:\n","            print(\"No metrics found.\")\n","        if current_f1 is not None and current_f1 > self.best_f1 and not self.testing:\n","            self.best_f1 = current_f1\n","            print(f\"New best F1 score: {current_f1}. Saving model...\")\n","            model.save_adapter(\"DutchAdapter\\\\NLU prompt 4 babylm\\\\FineTune NLI\\\\best_model\", adapter, with_head=True)\n","            # Perform evaluation with test set when we have a new best model\n","            print(\"Evaluating model on the test set...\")\n","            self.testing = True\n","            eval_result = trainer.evaluate(self.test_dataset)\n","            self.testing = False\n","            print(f\"scores on test set for best model: {eval_result}\")\n","\n","# Define the adapter arguments\n","adapterArgs = adapters.training.AdapterArguments(\n","  train_adapter=True,\n",")\n","\n","# Setup the adapter training (this will add the adapter to the model)\n","print(\"Setting up adapter training...\")\n","adapters.training.setup_adapter_training(\n","  model=model,\n","  adapter_args=adapterArgs,\n","  adapter_name=adapter,\n",")\n","\n","# Define the training arguments for finetuning. Default training arguments from the adapter library are used\n","trainingArgs = TrainingArguments(\n","  learning_rate=1e-4,\n","  num_train_epochs=20,\n","  output_dir=\"DutchAdapter\\\\NLU prompt 4 babylm\\\\FineTune NLI\",\n","  evaluation_strategy=\"epoch\",\n","  save_strategy=\"no\"  # Disable automatic saving, we will handle it in the callback\n",")\n","\n","trainer = adapters.AdapterTrainer(\n","        model=model,\n","        args=trainingArgs,\n","        train_dataset=train_dataset,\n","        eval_dataset=validation_dataset,\n","        tokenizer=tokenizer,\n","        compute_metrics=compute_metrics,\n","        callbacks=[SaveBestModelCallback(test_dataset)]\n","    )\n","\n","print(\"Training the model...\")\n","trainer.train()\n","\n","# Save the final model\n","print(\"Saving the final model...\")\n","model.save_adapter(\"DutchAdapter\\\\NLU prompt 4 babylm\\\\FineTune NLI\\\\final_model\", adapter, with_head=True)\n","\n","# Evaluate the final model\n","print(\"Evaluating the model...\")\n","trainer.evaluate(test_dataset)\n"]},{"cell_type":"markdown","metadata":{"id":"qnL2PMDDEpHL"},"source":["# Save the best model as zip, so we can download it from the notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEH1-UV6EDIr","outputId":"d1c7ad15-e1dd-4afb-c052-c299751ae993"},"outputs":[{"name":"stdout","output_type":"stream","text":["updating: content/DutchAdapter\\NLU prompt 4 babylm\\FineTune NLI\\best_model/ (stored 0%)\n","\n","updating: content/DutchAdapter\\NLU prompt 4 babylm\\FineTune NLI\\best_model/pytorch_adapter.bin (deflated 7%)\n","\n","updating: content/DutchAdapter\\NLU prompt 4 babylm\\FineTune NLI\\best_model/adapter_config.json (deflated 60%)\n"]}],"source":["!zip -r bestModel.zip \"/content/DutchAdapter\\NLU prompt 4 babylm\\FineTune NLI\\best_model\""]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":4}
